import asyncio
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from mcp_clients.ollama_client import MCPOllamaClient
from config import (
    MCP_OLLAMA_SSH_HOST,
    MCP_OLLAMA_SSH_PORT,
    MCP_OLLAMA_SSH_USER,
    MCP_OLLAMA_SSH_KEY,
    MCP_OLLAMA_NODE_PATH,
    MCP_OLLAMA_SERVER_PATH
)

async def rebuild():
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –Ω—É–ª—è"""
    
    print("üîå –ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ Ollama MCP...")
    client = MCPOllamaClient(
        ssh_host=MCP_OLLAMA_SSH_HOST,
        ssh_port=MCP_OLLAMA_SSH_PORT,
        ssh_user=MCP_OLLAMA_SSH_USER,
        ssh_key=MCP_OLLAMA_SSH_KEY,
        node_path=MCP_OLLAMA_NODE_PATH,
        server_path=MCP_OLLAMA_SERVER_PATH
    )
    await client.start()
    
    print("üîÑ –ß–∏—Ç–∞—é FAQ —Ñ–∞–π–ª...")
    with open('support_faq.txt', 'r', encoding='utf-8') as f:
        faq_text = f.read()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –±–ª–æ–∫–∏
    print("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞—é FAQ –Ω–∞ Q&A –ø–∞—Ä—ã...")
    
    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –¥–≤–æ–π–Ω–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É —Å—Ç—Ä–æ–∫–∏
    blocks = [b.strip() for b in faq_text.split('\n\n') if b.strip() and len(b.strip()) > 50]
    
    print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(blocks)} –±–ª–æ–∫–æ–≤")
    
    # –°–æ–∑–¥–∞—ë–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π chunk_and_embed –¥–ª—è –≤—Å–µ–≥–æ FAQ
    # –Ω–æ —Å –º–µ–Ω—å—à–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
    print("üß† –°–æ–∑–¥–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    
    all_chunks_data = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 5 –±–ª–æ–∫–æ–≤ –∑–∞ —Ä–∞–∑
    for i in range(0, len(blocks), 5):
        batch = blocks[i:i+5]
        batch_text = "\n\n---\n\n".join(batch)
        
        print(f"  üìù Batch {i//5 + 1}/{(len(blocks)-1)//5 + 1}...", end='\r')
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º rag_answer –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        # –≠—Ç–æ –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–π —Å–ø–æ—Å–æ–±
        result = await client.call_tool("chunk_and_embed", {
            "text": batch_text,
            "chunk_size": 800,
            "chunk_overlap": 50
        })
        
        if result and 'chunks' in result:
            all_chunks_data.extend(result['chunks'])
    
    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_chunks_data)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    if not all_chunks_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
        await client.stop()
        return
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ù–û–í–û–ï —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤ bot_knowledge...")
    save_result = await client.call_tool("vector_store_save", {
        "name": "bot_knowledge",
        "chunks": all_chunks_data
    })
    
    if save_result:
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_chunks_data)} —á–∞–Ω–∫–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        print("üîç –ü—Ä–æ–≤–µ—Ä—è—é –∑–∞–≥—Ä—É–∑–∫—É...")
        load_result = await client.call_tool("vector_store_load", {
            "name": "bot_knowledge"
        })
        
        if load_result and 'chunks' in load_result:
            print(f"‚úÖ –•—Ä–∞–Ω–∏–ª–∏—â–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è: {len(load_result['chunks'])} —á–∞–Ω–∫–æ–≤")
        else:
            print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π: {load_result}")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    await client.stop()
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")

if __name__ == "__main__":
    asyncio.run(rebuild())
