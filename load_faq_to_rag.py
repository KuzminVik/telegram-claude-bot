import asyncio
import sys
import os
import re

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from mcp_clients.ollama_client import MCPOllamaClient
from config import (
    MCP_OLLAMA_SSH_HOST,
    MCP_OLLAMA_SSH_PORT,
    MCP_OLLAMA_SSH_USER,
    MCP_OLLAMA_SSH_KEY,
    MCP_OLLAMA_NODE_PATH,
    MCP_OLLAMA_SERVER_PATH
)

async def load_faq():
    """Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ FAQ Ð² Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ"""
    
    print("ðŸ”Œ ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÑŽÑÑŒ Ðº Ollama MCP...")
    client = MCPOllamaClient(
        ssh_host=MCP_OLLAMA_SSH_HOST,
        ssh_port=MCP_OLLAMA_SSH_PORT,
        ssh_user=MCP_OLLAMA_SSH_USER,
        ssh_key=MCP_OLLAMA_SSH_KEY,
        node_path=MCP_OLLAMA_NODE_PATH,
        server_path=MCP_OLLAMA_SERVER_PATH
    )
    await client.start()
    
    print("ðŸ”„ Ð§Ð¸Ñ‚Ð°ÑŽ FAQ Ñ„Ð°Ð¹Ð»...")
    with open('support_faq.txt', 'r', encoding='utf-8') as f:
        faq_text = f.read()
    
    print(f"ðŸ“„ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(faq_text)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
    
    # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° ÑÐµÐºÑ†Ð¸Ð¸ Ð¿Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼ ###
    print("âœ‚ï¸ Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÑŽ Ð½Ð° ÑÐµÐºÑ†Ð¸Ð¸...")
    sections = re.split(r'\n### ', faq_text)
    
    # ÐŸÐµÑ€Ð²Ð°Ñ ÑÐµÐºÑ†Ð¸Ñ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº ##, Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
    all_chunks = []
    
    for i, section in enumerate(sections):
        if not section.strip():
            continue
        
        # Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº
        if i > 0:
            section = "### " + section
        
        # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ ÑÐµÐºÑ†Ð¸Ð¸ Ð´Ð¾ 800 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        if len(section) > 800:
            # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾ Ð°Ð±Ð·Ð°Ñ†Ð°Ð¼
            paragraphs = section.split('\n\n')
            current_chunk = ""
            
            for para in paragraphs:
                if len(current_chunk) + len(para) < 800:
                    current_chunk += para + "\n\n"
                else:
                    if current_chunk.strip():
                        all_chunks.append(current_chunk.strip())
                    current_chunk = para + "\n\n"
            
            if current_chunk.strip():
                all_chunks.append(current_chunk.strip())
        else:
            all_chunks.append(section.strip())
    
    print(f"âœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ {len(all_chunks)} ÑÐµÐºÑ†Ð¸Ð¹ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ")
    
    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‡Ð°Ð½ÐºÐ°
    print("ðŸ§  Ð¡Ð¾Ð·Ð´Ð°ÑŽ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÐµÐºÑ†Ð¸Ð¸...")
    embedded_chunks = []
    
    for idx, chunk_text in enumerate(all_chunks):
        print(f"  ðŸ“ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ {idx+1}/{len(all_chunks)}...", end='\r')
        
        result = await client.call_tool("chunk_and_embed", {
            "text": chunk_text,
            "chunk_size": 2000,  # Ð‘Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€, Ñ‚.Ðº. ÑƒÐ¶Ðµ Ñ€Ð°Ð·Ð±Ð¸Ð»Ð¸
            "chunk_overlap": 0
        })
        
        if result and 'chunks' in result:
            embedded_chunks.extend(result['chunks'])
    
    print(f"\nâœ… Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ {len(embedded_chunks)} ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²")
    
    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ
    print("ðŸ’¾ Ð”Ð¾Ð±Ð°Ð²Ð»ÑÑŽ Ð² Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ bot_knowledge...")
    load_result = await client.call_tool("vector_store_load", {
        "name": "bot_knowledge"
    })
    
    existing_chunks = load_result.get('chunks', []) if load_result else []
    print(f"ðŸ“¦ Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²: {len(existing_chunks)}")
    
    # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼
    all_embedded = existing_chunks + embedded_chunks
    
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼
    save_result = await client.call_tool("vector_store_save", {
        "name": "bot_knowledge",
        "chunks": all_embedded
    })
    
    if save_result:
        print(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾ {len(all_embedded)} Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð² bot_knowledge")
        print(f"ðŸ“Š Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ Ð½Ð¾Ð²Ñ‹Ñ…: {len(embedded_chunks)}")
    else:
        print("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ")
    
    await client.stop()
    print("âœ… Ð“Ð¾Ñ‚Ð¾Ð²Ð¾!")

if __name__ == "__main__":
    asyncio.run(load_faq())
